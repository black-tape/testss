# web_search_integration.py - ç½‘ç»œæ£€ç´¢é›†æˆæ¨¡å—

import requests
import re
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from urllib.parse import quote, urlparse
from langchain_core.documents import Document
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WebSearchEngine:
    """ç½‘ç»œæœç´¢å¼•æ“åŸºç±»"""

    def search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢ç½‘ç»œå†…å®¹"""
        raise NotImplementedError


class DuckDuckGoSearchEngine(WebSearchEngine):
    """DuckDuckGo æœç´¢å¼•æ“ - å…è´¹ä¸”æ— éœ€APIå¯†é’¥"""

    def search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """ä½¿ç”¨DuckDuckGoè¿›è¡Œæœç´¢"""
        try:
            # DuckDuckGo Instant Answer API
            url = "https://api.duckduckgo.com/"
            params = {
                "q": query,
                "format": "json",
                "no_html": 1,
                "skip_disambig": 1
            }

            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()

            results = []

            # ä¸»è¦ç»“æœ
            if data.get("Abstract"):
                results.append({
                    "title": data.get("Heading", ""),
                    "content": data["Abstract"],
                    "url": data.get("AbstractURL", ""),
                    "source": "DuckDuckGo Abstract"
                })

            # ç›¸å…³ä¸»é¢˜
            for topic in data.get("RelatedTopics", [])[:max_results]:
                if "Text" in topic:
                    results.append({
                        "title": topic.get("FirstURL", "").split("/")[-1].replace("_", " "),
                        "content": topic["Text"],
                        "url": topic.get("FirstURL", ""),
                        "source": "DuckDuckGo Related"
                    })

            return results[:max_results]

        except Exception as e:
            logger.error(f"DuckDuckGoæœç´¢å¤±è´¥: {e}")
            return []


class WikipediaSearchEngine(WebSearchEngine):
    """ç»´åŸºç™¾ç§‘æœç´¢å¼•æ“ - é€‚åˆå­¦æœ¯å†…å®¹"""

    def search(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """æœç´¢ç»´åŸºç™¾ç§‘å†…å®¹"""
        try:
            # æœç´¢ç»´åŸºç™¾ç§‘é¡µé¢
            search_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{quote(query)}"

            response = requests.get(search_url, timeout=10)

            if response.status_code == 200:
                data = response.json()

                return [{
                    "title": data.get("title", ""),
                    "content": data.get("extract", ""),
                    "url": data.get("content_urls", {}).get("desktop", {}).get("page", ""),
                    "source": "Wikipedia"
                }]
            else:
                # å¦‚æœç›´æ¥è®¿é—®å¤±è´¥ï¼Œå°è¯•æœç´¢
                return self._search_wikipedia_fallback(query, max_results)

        except Exception as e:
            logger.error(f"ç»´åŸºç™¾ç§‘æœç´¢å¤±è´¥: {e}")
            return []

    def _search_wikipedia_fallback(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """ç»´åŸºç™¾ç§‘æœç´¢åå¤‡æ–¹æ¡ˆ"""
        try:
            search_url = "https://en.wikipedia.org/w/api.php"
            params = {
                "action": "query",
                "list": "search",
                "srsearch": query,
                "utf8": 1,
                "format": "json",
                "srlimit": max_results
            }

            response = requests.get(search_url, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()
            results = []

            for item in data.get("query", {}).get("search", []):
                title = item.get("title", "")
                page_id = item.get("pageid", 0)

                if page_id:
                    # è·å–é¡µé¢æ‘˜è¦
                    summary_url = "https://en.wikipedia.org/w/api.php"
                    summary_params = {
                        "action": "query",
                        "prop": "extracts",
                        "exintro": 1,
                        "explaintext": 1,
                        "pageids": page_id,
                        "utf8": 1,
                        "format": "json"
                    }

                    summary_response = requests.get(summary_url, params=summary_params, timeout=5)
                    if summary_response.status_code == 200:
                        summary_data = summary_response.json()
                        pages = summary_data.get("query", {}).get("pages", {})

                        if str(page_id) in pages:
                            page = pages[str(page_id)]
                            content = page.get("extract", "")

                            if content:
                                results.append({
                                    "title": title,
                                    "content": content,
                                    "url": f"https://en.wikipedia.org/wiki/{quote(title)}",
                                    "source": "Wikipedia"
                                })

            return results

        except Exception as e:
            logger.error(f"ç»´åŸºç™¾ç§‘åå¤‡æœç´¢å¤±è´¥: {e}")
            return []


class WebContentExtractor:
    """ç½‘é¡µå†…å®¹æå–å™¨"""

    @staticmethod
    def extract_content(url: str, max_length: int = 2000) -> str:
        """æå–ç½‘é¡µä¸»è¦å†…å®¹"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()

            # æå–ä¸»è¦å†…å®¹
            content = ""

            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|body'))

            if main_content:
                content = main_content.get_text(separator=' ', strip=True)
            else:
                # æå–æ‰€æœ‰æ®µè½
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs])

            # æ¸…ç†å†…å®¹
            content = re.sub(r'\s+', ' ', content)
            content = content.strip()

            return content[:max_length] + "..." if len(content) > max_length else content

        except Exception as e:
            logger.error(f"ç½‘é¡µå†…å®¹æå–å¤±è´¥ {url}: {e}")
            return ""


class HybridRAGSystem:
    """æ··åˆRAGç³»ç»Ÿ - ç»“åˆæœ¬åœ°çŸ¥è¯†åº“å’Œç½‘ç»œæ£€ç´¢"""

    def __init__(self, local_retriever, enable_web_search: bool = True):
        self.local_retriever = local_retriever
        self.enable_web_search = enable_web_search

        # åˆå§‹åŒ–æœç´¢å¼•æ“
        self.search_engines = [
            DuckDuckGoSearchEngine(),
            WikipediaSearchEngine()
        ]

        self.content_extractor = WebContentExtractor()

    def search_and_retrieve(self, query: str, use_local: bool = True, use_web: bool = True) -> List[Document]:
        """
        æ··åˆæ£€ç´¢ï¼šç»“åˆæœ¬åœ°å’Œç½‘ç»œæœç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            use_local: æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ£€ç´¢
            use_web: æ˜¯å¦ä½¿ç”¨ç½‘ç»œæ£€ç´¢

        Returns:
            åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
        """
        all_docs = []

        # 1. æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢
        if use_local and self.local_retriever:
            try:
                local_docs = self.local_retriever.get_relevant_documents(query, method="enhanced")
                for doc in local_docs:
                    doc.metadata.update({
                        "source_type": "local",
                        "retrieval_method": "vector_search"
                    })
                all_docs.extend(local_docs)
                logger.info(f"æœ¬åœ°æ£€ç´¢åˆ° {len(local_docs)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.error(f"æœ¬åœ°æ£€ç´¢å¤±è´¥: {e}")

        # 2. ç½‘ç»œæœç´¢
        if use_web and self.enable_web_search:
            web_docs = self._web_search(query)
            all_docs.extend(web_docs)
            logger.info(f"ç½‘ç»œæœç´¢åˆ° {len(web_docs)} ä¸ªæ–‡æ¡£")

        # 3. å»é‡å’Œæ’åº
        return self._deduplicate_and_rank(all_docs)

    def _web_search(self, query: str, max_results_per_engine: int = 2) -> List[Document]:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        web_docs = []

        # ä¸ºè‹±è¯­å­¦ä¹ æ·»åŠ ç›¸å…³å…³é”®è¯
        enhanced_query = self._enhance_query(query)

        for engine in self.search_engines:
            try:
                results = engine.search(enhanced_query, max_results_per_engine)

                for result in results:
                    content = result.get("content", "")
                    title = result.get("title", "")
                    url = result.get("url", "")
                    source = result.get("source", "web")

                    if content and len(content) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                        doc = Document(
                            page_content=content,
                            metadata={
                                "source": url,
                                "title": title,
                                "source_type": "web",
                                "engine": source,
                                "query": enhanced_query
                            }
                        )
                        web_docs.append(doc)

                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)

            except Exception as e:
                logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥ {engine.__class__.__name__}: {e}")

        return web_docs

    def _enhance_query(self, query: str) -> str:
        """ä¸ºè‹±è¯­å­¦ä¹ æŸ¥è¯¢æ·»åŠ ç›¸å…³å…³é”®è¯"""
        english_keywords = [
            "English grammar", "ESL", "English learning",
            "English usage", "grammar rules"
        ]

        # æ£€æŸ¥æ˜¯å¦æ˜¯è‹±è¯­å­¦ä¹ ç›¸å…³æŸ¥è¯¢
        if any(keyword in query.lower() for keyword in ["è‹±è¯­", "è¯­æ³•", "ç”¨æ³•", "æ—¶æ€", "å† è¯"]):
            # ä¸ºç®€å•çš„ä¸­æ–‡æŸ¥è¯¢æ·»åŠ è‹±æ–‡å…³é”®è¯
            if len(query) < 20:
                return f"{query} English grammar rules examples"

        return query

    def _deduplicate_and_rank(self, docs: List[Document]) -> List[Document]:
        """å»é‡å’Œæ’åºæ–‡æ¡£"""
        # ç®€å•å»é‡ï¼šåŸºäºå†…å®¹å‰100å­—ç¬¦
        unique_docs = []
        seen_content = set()

        # ä¼˜å…ˆæœ¬åœ°æ–‡æ¡£ï¼Œç„¶åç½‘ç»œæ–‡æ¡£
        local_docs = [doc for doc in docs if doc.metadata.get("source_type") == "local"]
        web_docs = [doc for doc in docs if doc.metadata.get("source_type") == "web"]

        # å…ˆæ·»åŠ æœ¬åœ°æ–‡æ¡£
        for doc in local_docs:
            content = doc.page_content[:100]
            if content not in seen_content:
                seen_content.add(content)
                unique_docs.append(doc)

        # å†æ·»åŠ ç½‘ç»œæ–‡æ¡£
        for doc in web_docs:
            content = doc.page_content[:100]
            if content not in seen_content:
                seen_content.add(content)
                unique_docs.append(doc)

        return unique_docs[:8]  # é™åˆ¶æ€»æ–‡æ¡£æ•°é‡


def create_hybrid_retriever(local_retriever, enable_web_search: bool = True) -> HybridRAGSystem:
    """åˆ›å»ºæ··åˆæ£€ç´¢å™¨"""
    return HybridRAGSystem(local_retriever, enable_web_search)


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # æµ‹è¯•ç½‘ç»œæœç´¢åŠŸèƒ½
    print("ğŸŒ æµ‹è¯•ç½‘ç»œæœç´¢åŠŸèƒ½")
    print("=" * 50)

    # æµ‹è¯•æœç´¢å¼•æ“
    ddg = DuckDuckGoSearchEngine()
    wiki = WikipediaSearchEngine()

    test_query = "ç°åœ¨å®Œæˆæ—¶ English grammar"

    print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {test_query}")

    # DuckDuckGoæœç´¢
    print("\nğŸ“Š DuckDuckGoæœç´¢ç»“æœ:")
    ddg_results = ddg.search(test_query)
    for i, result in enumerate(ddg_results[:3], 1):
        print(f"{i}. {result['title']}")
        print(f"   {result['content'][:100]}...")
        print(f"   æ¥æº: {result['source']}")
        print()

    # ç»´åŸºç™¾ç§‘æœç´¢
    print("\nğŸ“š ç»´åŸºç™¾ç§‘æœç´¢ç»“æœ:")
    wiki_results = wiki.search(test_query)
    for i, result in enumerate(wiki_results[:2], 1):
        print(f"{i}. {result['title']}")
        print(f"   {result['content'][:100]}...")
        print(f"   æ¥æº: {result['source']}")
        print()

    print("âœ… ç½‘ç»œæœç´¢åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")