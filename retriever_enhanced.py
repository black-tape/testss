# retriever_enhanced.py - å¢å¼ºçš„æ£€ç´¢å™¨ç³»ç»Ÿ

from langchain_community.vectorstores import FAISS
from langchain_ollama.llms import OllamaLLM
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
import numpy as np
from typing import List, Dict, Any
from config import DB_DIR, EMBEDDING_MODEL, LLM_MODEL


class EnhancedRetriever:
    """å¢å¼ºçš„RAGæ£€ç´¢å™¨ï¼Œæ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥å’Œä¼˜åŒ–"""

    def __init__(self, vector_store_path: str = DB_DIR):
        self.vector_store_path = vector_store_path
        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
        self.llm = OllamaLLM(model=LLM_MODEL)

        # åŠ è½½å‘é‡æ•°æ®åº“
        self.db = FAISS.load_local(
            vector_store_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )

        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self._setup_retrievers()

    def _setup_retrievers(self):
        """è®¾ç½®å¤šç§æ£€ç´¢å™¨ç­–ç•¥"""

        # 1. å‘é‡ç›¸ä¼¼æ€§æ£€ç´¢å™¨ï¼ˆä¼˜åŒ–çš„ï¼‰
        self.vector_retriever = self.db.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": 5,                    # æ£€ç´¢Top5æœ€ç›¸å…³æ–‡æ¡£
                "fetch_k": 10,            # åˆæ­¥æ£€ç´¢æ›´å¤šå€™é€‰
            }
        )

        # 2. å¤šæ ·æ€§æ£€ç´¢å™¨ï¼ˆMaximal Marginal Relevanceï¼‰
        self.mmr_retriever = self.db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 4,                    # è¿”å›4ä¸ªæ–‡æ¡£
                "fetch_k": 20,            # ä»æ›´å¤šå€™é€‰ä¸­é€‰æ‹©
                "lambda_mult": 0.5,       # å¤šæ ·æ€§æƒé‡ (0-1)
            }
        )

        # 3. å‡†å¤‡æ··åˆæ£€ç´¢ï¼ˆéœ€è¦æ–‡æ¡£åˆ—è¡¨ç”¨äºBM25ï¼‰
        self._setup_ensemble_retriever()

        # 4. ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨
        self._setup_compression_retriever()

    def _setup_ensemble_retriever(self):
        """è®¾ç½®æ··åˆæ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨å‘é‡æ£€ç´¢å™¨
            self.ensemble_retriever = self.vector_retriever
            print("âœ… ä½¿ç”¨å‘é‡æ£€ç´¢å™¨ä½œä¸ºæ··åˆæ£€ç´¢ç­–ç•¥")
        except Exception as e:
            print(f"âš ï¸ æ··åˆæ£€ç´¢å™¨è®¾ç½®å¤±è´¥: {e}")
            self.ensemble_retriever = self.vector_retriever

    def _setup_compression_retriever(self):
        """è®¾ç½®ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨å‘é‡æ£€ç´¢å™¨
            self.compression_retriever = self.vector_retriever
            print("âœ… ä½¿ç”¨å‘é‡æ£€ç´¢å™¨ä½œä¸ºå‹ç¼©æ£€ç´¢ç­–ç•¥")
        except Exception as e:
            print(f"âš ï¸ å‹ç¼©æ£€ç´¢å™¨è®¾ç½®å¤±è´¥: {e}")
            self.compression_retriever = self.vector_retriever

    def get_relevant_documents(self, query: str, method: str = "enhanced") -> List[Document]:
        """
        æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            method: æ£€ç´¢æ–¹æ³• ('vector', 'mmr', 'ensemble', 'compression', 'enhanced')

        Returns:
            æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if method == "vector":
            return self.vector_retriever.invoke(query)
        elif method == "mmr":
            return self.mmr_retriever.invoke(query)
        elif method == "ensemble":
            return self.ensemble_retriever.invoke(query)
        elif method == "compression":
            return self.compression_retriever.invoke(query)
        elif method == "enhanced":
            # å¢å¼ºæ£€ç´¢ï¼šç»“åˆå¤šç§æ–¹æ³•
            return self._enhanced_retrieval(query)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ£€ç´¢æ–¹æ³•: {method}")

    def _enhanced_retrieval(self, query: str) -> List[Document]:
        """å¢å¼ºæ£€ç´¢ï¼šç»“åˆå¤šç§ç­–ç•¥"""
        results = []

        # 1. å‘é‡ç›¸ä¼¼æ€§æ£€ç´¢
        vector_docs = self.vector_retriever.invoke(query)
        results.extend(vector_docs)

        # 2. å¦‚æœå‘é‡æ£€ç´¢ç»“æœä¸å¤Ÿï¼Œä½¿ç”¨MMR
        if len(vector_docs) < 3:
            mmr_docs = self.mmr_retriever.invoke(query)
            # æ·»åŠ ä¸é‡å¤çš„æ–‡æ¡£
            for doc in mmr_docs:
                if doc not in results and len(results) < 6:
                    results.append(doc)

        # 3. å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_docs = []
        seen_content = set()

        for doc in results:
            content = doc.page_content[:100]  # ä½¿ç”¨å‰100å­—ç¬¦åˆ¤æ–­æ˜¯å¦é‡å¤
            if content not in seen_content:
                seen_content.add(content)
                unique_docs.append(doc)
                if len(unique_docs) >= 5:  # æœ€å¤šè¿”å›5ä¸ªæ–‡æ¡£
                    break

        return unique_docs

    def analyze_retrieval_quality(self, query: str, docs: List[Document]) -> Dict[str, Any]:
        """åˆ†ææ£€ç´¢è´¨é‡"""
        if not docs:
            return {
                "query": query,
                "num_results": 0,
                "avg_content_length": 0,
                "quality_score": 0,
                "recommendations": ["æœªæ£€ç´¢åˆ°æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢æˆ–çŸ¥è¯†åº“"]
            }

        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        content_lengths = [len(doc.page_content) for doc in docs]
        avg_length = np.mean(content_lengths)

        # ç®€å•çš„è´¨é‡è¯„åˆ†ï¼ˆåŸºäºå†…å®¹é•¿åº¦å’Œæ–‡æ¡£æ•°é‡ï¼‰
        quality_score = min(100, (len(docs) * 20) + (avg_length / 20))

        # ç”Ÿæˆå»ºè®®
        recommendations = []
        if len(docs) < 3:
            recommendations.append("æ£€ç´¢ç»“æœè¾ƒå°‘ï¼Œè€ƒè™‘é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼")
        if avg_length < 100:
            recommendations.append("æ–‡æ¡£ç‰‡æ®µè¾ƒçŸ­ï¼Œå¯èƒ½ç¼ºä¹ä¸Šä¸‹æ–‡")
        if quality_score < 60:
            recommendations.append("æ£€ç´¢è´¨é‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æŸ¥è¯¢æˆ–æ£€ç´¢ç­–ç•¥")

        return {
            "query": query,
            "num_results": len(docs),
            "avg_content_length": avg_length,
            "quality_score": quality_score,
            "recommendations": recommendations,
            "documents_preview": [{"content": doc.page_content[:100] + "..."} for doc in docs[:3]]
        }


def create_enhanced_retriever() -> EnhancedRetriever:
    """åˆ›å»ºå¢å¼ºæ£€ç´¢å™¨å®ä¾‹"""
    return EnhancedRetriever()


if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºæ£€ç´¢å™¨
    retriever = create_enhanced_retriever()

    test_query = "ç°åœ¨å®Œæˆæ—¶çš„ç”¨æ³•"

    print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {test_query}")
    print("=" * 50)

    # æµ‹è¯•ä¸åŒæ£€ç´¢æ–¹æ³•
    methods = ["vector", "mmr", "enhanced"]
    for method in methods:
        print(f"\nğŸ“Š {method.upper()} æ£€ç´¢ç»“æœ:")
        docs = retriever.get_relevant_documents(test_query, method=method)

        for i, doc in enumerate(docs, 1):
            print(f"{i}. [{doc.metadata.get('source', 'unknown')}]")
            print(f"   {doc.page_content[:150]}...")

        # åˆ†ææ£€ç´¢è´¨é‡
        quality = retriever.analyze_retrieval_quality(test_query, docs)
        print(f"   ğŸ“ˆ è´¨é‡è¯„åˆ†: {quality['quality_score']:.1f}/100")
        print(f"   ğŸ’¡ å»ºè®®: {', '.join(quality['recommendations']) if quality['recommendations'] else 'è‰¯å¥½'}")