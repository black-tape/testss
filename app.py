from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama.llms import OllamaLLM
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
import gradio as gr
from config import DB_DIR, EMBEDDING_MODEL, LLM_MODEL, QA_PROMPT

# 1ï¸âƒ£ å‘é‡æ•°æ®åº“
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
db = FAISS.load_local(DB_DIR, embeddings, allow_dangerous_deserialization=True)
retriever = db.as_retriever()

# 2ï¸âƒ£ LLM
llm = OllamaLLM(model=LLM_MODEL)

# 3ï¸âƒ£ Prompt
prompt = PromptTemplate(
    template=QA_PROMPT,
    input_variables=["context", "query"]
)

# 4ï¸âƒ£ QA Chain (ä½¿ç”¨ç°ä»£ LCEL è¯­æ³•)
qa_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
)

# -----------------------------
# 5ï¸âƒ£ å®šä¹‰å‰ç«¯å‡½æ•°
# -----------------------------
def chat_with_agent(query):
    if not query.strip():
        return "âŒ è¯·è¾“å…¥é—®é¢˜"
    try:
        result = qa_chain.invoke(query)
        return result
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}"

# -----------------------------
# 4ï¸âƒ£ Gradio å‰ç«¯ç¾åŒ–
# -----------------------------
with gr.Blocks(title="ğŸ“ è‹±è¯­å­¦ä¹ æ™ºèƒ½ä½“") as demo:
    gr.Markdown(
        """
        # ğŸ“ è‹±è¯­å­¦ä¹ æ™ºèƒ½ä½“
        
        > ğŸ’¡ è¾“å…¥ä¸€ä¸ªè‹±è¯­é—®é¢˜æˆ–è¯­æ³•ç‚¹ï¼Œæˆ‘ä¼šç»“åˆæ–‡æ¡£ä¸ºä½ è®²è§£ã€‚
        """
    )
    
    with gr.Row():
        query = gr.Textbox(
            label="ä½ çš„é—®é¢˜",
            placeholder="ä¾‹å¦‚ï¼šç°åœ¨å®Œæˆæ—¶çš„è€ƒç‚¹",
            lines=2
        )
        submit = gr.Button("æé—®", variant="primary")
    
    output = gr.Markdown(label="å›ç­”")
    submit.click(fn=chat_with_agent, inputs=query, outputs=output)

# -----------------------------
# 5ï¸âƒ£ å¯åŠ¨
# -----------------------------
if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=7862)
