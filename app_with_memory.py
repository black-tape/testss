# app_with_memory.py - å¸¦ä¸Šä¸‹æ–‡è®°å¿†åŠŸèƒ½çš„RAGåº”ç”¨

from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama.llms import OllamaLLM
from retriever_enhanced import EnhancedRetriever
from conversation_memory import ConversationMemory, get_conversation_memory
import gradio as gr
from config import LLM_MODEL
import time
from typing import Tuple, Dict, Any, List
import json


class ConversationRAGApp:
    """å¸¦ä¸Šä¸‹æ–‡è®°å¿†çš„RAGåº”ç”¨"""

    def __init__(self):
        # 1ï¸âƒ£ æ£€ç´¢å™¨
        self.retriever = EnhancedRetriever()

        # 2ï¸âƒ£ å¯¹è¯è®°å¿†
        self.memory = get_conversation_memory()

        # 3ï¸âƒ£ LLM
        self.llm = OllamaLLM(model=LLM_MODEL)

        # 4ï¸âƒ£ å¢å¼ºçš„Promptæ¨¡æ¿ï¼ˆæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†ï¼‰
        self.prompt_with_context = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­å­¦ä¹ åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£å’Œå¯¹è¯å†å²ï¼Œå…¨é¢å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼š**
{conversation_context}

**æ£€ç´¢åˆ°çš„å­¦ä¹ èµ„æ–™ï¼š**
{retrieved_docs}

**å½“å‰ç”¨æˆ·é—®é¢˜ï¼š**
{query}

**å›ç­”è¦æ±‚ï¼š**
1. ä»”ç»†åˆ†æå¯¹è¯å†å²ï¼Œç†è§£ç”¨æˆ·çš„èƒŒæ™¯å’Œä¹‹å‰çš„è®¨è®º
2. ç»“åˆæ£€ç´¢åˆ°çš„èµ„æ–™ç»™å‡ºå‡†ç¡®çš„è¯­æ³•è§£é‡Š
3. å¦‚æœç”¨æˆ·åœ¨è¿½é—®ï¼Œè¯·åŸºäºä¹‹å‰çš„å›ç­”è¿›è¡Œè¡¥å……è¯´æ˜
4. æä¾›å…·ä½“çš„è‹±æ–‡ç¤ºä¾‹å’Œç”¨æ³•è¯´æ˜
5. ä¿æŒå›ç­”çš„è¿è´¯æ€§å’Œä¸€è‡´æ€§
6. å¦‚æœå‘ç°äº†ä¹‹å‰å¯èƒ½çš„é”™è¯¯ï¼Œè¯·ä¸»åŠ¨çº æ­£å’Œæ¾„æ¸…

**å›ç­”ï¼š**""",
            input_variables=["conversation_context", "retrieved_docs", "query"]
        )

        # 5ï¸âƒ£ å¤„ç†é“¾
        self.qa_chain = (
            {
                "conversation_context": lambda q: self.memory.get_context_for_query(q),
                "retrieved_docs": lambda q: self._get_retrieved_docs(q),
                "query": RunnablePassthrough()
            }
            | self.prompt_with_context
            | self.llm
        )

    def _get_retrieved_docs(self, query: str) -> str:
        """è·å–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
        try:
            docs = self.retriever.get_relevant_documents(query, method="enhanced")
            if not docs:
                return "æœªæ‰¾åˆ°ç›¸å…³çš„è‹±è¯­å­¦ä¹ èµ„æ–™ã€‚"

            formatted_docs = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get('source', f'æ–‡æ¡£ç‰‡æ®µ{i}')
                content = doc.page_content.strip()[:800]  # é™åˆ¶é•¿åº¦
                formatted_docs.append(f"[èµ„æ–™{i} - {source}]:\n{content}")

            return "\n\n".join(formatted_docs)
        except Exception as e:
            return f"æ£€ç´¢æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}"

    def chat_with_memory(self, query: str, use_memory: bool = True) -> Tuple[str, Dict[str, Any]]:
        """å¸¦è®°å¿†çš„å¯¹è¯"""
        if not query.strip():
            return "âŒ è¯·è¾“å…¥é—®é¢˜", {}

        try:
            start_time = time.time()

            # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
            conversation_context = ""
            if use_memory and self.memory.conversation_history:
                conversation_context = self.memory.get_context_for_query(query)

            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            docs = self.retriever.get_relevant_documents(query, method="enhanced")
            retrieval_quality = self.retriever.analyze_retrieval_quality(query, docs)

            # å¦‚æœå¯ç”¨è®°å¿†ä¸”æœ‰ä¸Šä¸‹æ–‡
            if use_memory and conversation_context:
                # ä½¿ç”¨å¸¦ä¸Šä¸‹æ–‡çš„å¤„ç†é“¾
                response = self.qa_chain.invoke(query)
            else:
                # ç®€å•å›ç­”ï¼ˆæ— è®°å¿†ï¼‰
                response = self._simple_answer(query, docs)

            response_time = time.time() - start_time

            # ä¿å­˜å¯¹è¯è®°å½•
            if use_memory:
                self.memory.add_conversation_turn(
                    user_query=query,
                    ai_response=response,
                    metadata={
                        "response_time": response_time,
                        "retrieval_quality": retrieval_quality.get("quality_score", 0),
                        "num_docs": len(docs)
                    },
                    retrieved_docs=[doc.metadata.get("source", "unknown") for doc in docs]
                )

            # æ·»åŠ è®°å¿†ä¿¡æ¯
            memory_info = self._format_memory_info(use_memory, conversation_context)
            final_response = response + "\n\n" + memory_info

            return final_response, {
                "retrieval_quality": retrieval_quality,
                "response_time": response_time,
                "num_retrieved_docs": len(docs),
                "memory_enabled": use_memory,
                "conversation_length": len(self.memory.conversation_history) if use_memory else 0
            }

        except Exception as e:
            return f"âŒ é”™è¯¯: {str(e)}", {}

    def _simple_answer(self, query: str, docs) -> str:
        """ç®€å•å›ç­”ï¼ˆä¸ä½¿ç”¨è®°å¿†ï¼‰"""
        if not docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„è‹±è¯­å­¦ä¹ èµ„æ–™ã€‚è¯·å°è¯•å…¶ä»–é—®é¢˜ã€‚"

        # ç®€å•çš„Prompt
        simple_prompt = PromptTemplate(
            template="""è¯·æ ¹æ®æä¾›çš„è‹±è¯­å­¦ä¹ èµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

èµ„æ–™ï¼š
{docs}

é—®é¢˜ï¼š{query}

è¯·ç»™å‡ºè¯¦ç»†å‡†ç¡®çš„å›ç­”ã€‚""",
            input_variables=["docs", "query"]
        )

        # æ ¼å¼åŒ–æ–‡æ¡£
        docs_text = "\n\n".join([f"èµ„æ–™{i+1}: {doc.page_content[:500]}..." for i, doc in enumerate(docs)])

        chain = simple_prompt | self.llm
        return chain.invoke({"docs": docs_text, "query": query})

    def _format_memory_info(self, use_memory: bool, context: str) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¿¡æ¯"""
        if not use_memory:
            return "ğŸ’­ **è®°å¿†åŠŸèƒ½**: å·²å…³é—­"

        info_parts = ["ğŸ’­ **è®°å¿†åŠŸèƒ½**: å·²å¯ç”¨"]

        if context:
            info_parts.append(f"ğŸ“ **å¯¹è¯å†å²**: å·²å‚è€ƒ {len(self.memory.conversation_history)} è½®å¯¹è¯")

        # æ˜¾ç¤ºæœ€è¿‘çš„è¯é¢˜
        if self.memory.conversation_history:
            recent_topics = self.memory.conversation_history[-1].keywords
            if recent_topics:
                info_parts.append(f"ğŸ·ï¸ **ç›¸å…³è¯é¢˜**: {', '.join(recent_topics[:3])}")

        return "\n".join(info_parts)

    def get_conversation_stats(self) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯"""
        return self.memory.get_conversation_stats()

    def clear_memory(self) -> str:
        """æ¸…ç©ºå¯¹è¯è®°å¿†"""
        self.memory.clear_history()
        return "âœ… å¯¹è¯å†å²å·²æ¸…ç©º"

    def export_conversation(self, format: str = "json") -> str:
        """å¯¼å‡ºå¯¹è¯å†å²"""
        try:
            return self.memory.export_conversation(format)
        except Exception as e:
            return f"å¯¼å‡ºå¤±è´¥: {str(e)}"


# åˆ›å»ºåº”ç”¨å®ä¾‹
app = ConversationRAGApp()


# Gradioç•Œé¢
with gr.Blocks(title="ğŸ§  æ™ºèƒ½è®°å¿†RAGè‹±è¯­åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ§  æ™ºèƒ½è®°å¿†RAGè‹±è¯­åŠ©æ‰‹

        > ğŸ’¡ **è®°ä½æ¯ä¸€æ¬¡å¯¹è¯** - å…·æœ‰ä¸Šä¸‹æ–‡è®°å¿†çš„æ™ºèƒ½è‹±è¯­å­¦ä¹ åŠ©æ‰‹

        ğŸ¯ **æ ¸å¿ƒç‰¹æ€§**:
        - ğŸ“š æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢
        - ğŸ§  å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†
        - ğŸ”„ è¿è´¯çš„å¤šè½®å¯¹è¯
        - ğŸ“Š æ™ºèƒ½è¯é¢˜è¿½è¸ª
        - ğŸ’¾ å¯¹è¯å†å²ç®¡ç†
        """
    )

    with gr.Row():
        with gr.Column(scale=3):
            query = gr.Textbox(
                label="ä½ çš„é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šåˆšæ‰æåˆ°çš„ç°åœ¨å®Œæˆæ—¶ï¼Œèƒ½ç»™æ›´å¤šä¾‹å­å—ï¼Ÿ",
                lines=3
            )

            with gr.Row():
                use_memory = gr.Checkbox(label="ğŸ§  å¯ç”¨è®°å¿†åŠŸèƒ½", value=True)
                memory_status = gr.Textbox(
                    label="è®°å¿†çŠ¶æ€",
                    value="å·²å¯ç”¨ - å¯è®°ä½å¯¹è¯å†å²",
                    interactive=False,
                    scale=2
                )

            with gr.Row():
                submit = gr.Button("ğŸ’¬ å¯¹è¯", variant="primary")
                clear_conversation = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºè®°å¿†", variant="secondary")
                export_btn = gr.Button("ğŸ“¥ å¯¼å‡ºå¯¹è¯", variant="secondary")

        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“ˆ å¯¹è¯ç»Ÿè®¡")
            with gr.Row():
                total_conversations = gr.Number(label="æ€»å¯¹è¯æ•°", precision=0, interactive=False)
                current_session = gr.Number(label="å½“å‰ä¼šè¯", precision=0, interactive=False)

            with gr.Row():
                avg_response_length = gr.Number(label="å¹³å‡å›ç­”é•¿åº¦", precision=0, interactive=False)

            gr.Markdown("### ğŸ·ï¸ å¸¸è§è¯é¢˜")
            topics_display = gr.Textbox(
                label="æœ€è¿‘è¯é¢˜",
                lines=3,
                interactive=False,
                placeholder="æš‚æ— è¯é¢˜è®°å½•"
            )

    # ä¸»å¯¹è¯åŒºåŸŸ
    with gr.Accordion("ğŸ’¬ æ™ºèƒ½å›ç­”", open=True):
        output = gr.Markdown(label="å›ç­”å†…å®¹")

    # å¯¹è¯è¯¦æƒ…åŒºåŸŸ
    with gr.Accordion("ğŸ“Š è¯¦ç»†ä¿¡æ¯", open=False):
        with gr.Row():
            quality_score = gr.Number(label="æ£€ç´¢è´¨é‡", precision=1, interactive=False)
            response_time = gr.Number(label="å“åº”æ—¶é—´(ç§’)", precision=2, interactive=False)
            docs_count = gr.Number(label="æ£€ç´¢æ–‡æ¡£æ•°", precision=0, interactive=False)

    # å¯¼å‡ºåŒºåŸŸ
    with gr.Accordion("ğŸ“‹ å¯¹è¯å¯¼å‡º", open=False):
        export_format = gr.Radio(
            choices=[("JSONæ ¼å¼", "json"), ("æ–‡æœ¬æ ¼å¼", "txt")],
            value="json",
            label="å¯¼å‡ºæ ¼å¼"
        )
        export_output = gr.Textbox(
            label="å¯¼å‡ºå†…å®¹",
            lines=10,
            interactive=False,
            show_copy_button=True
        )

    # ç»‘å®šäº‹ä»¶
    def process_query(query_text, memory_enabled):
        if not query_text.strip():
            return "âŒ è¯·è¾“å…¥é—®é¢˜", 0, 0, 0, 0, 0, ""

        response, metadata = app.chat_with_memory(query_text, memory_enabled)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats = app.get_conversation_stats()
        total_conv = stats["total_conversations"]
        current_sess = stats["current_session_length"]
        avg_length = stats["avg_response_length"]

        # æ ¼å¼åŒ–è¯é¢˜
        topics = stats["most_discussed_topics"]
        topics_text = ", ".join([f"{topic['topic']}({topic['count']}æ¬¡)" for topic in topics[:3]])

        # æ›´æ–°å…ƒæ•°æ®
        quality = metadata.get("retrieval_quality", {}).get("quality_score", 0)
        time_taken = metadata.get("response_time", 0)
        docs_num = metadata.get("num_retrieved_docs", 0)

        # æ›´æ–°è®°å¿†çŠ¶æ€
        memory_status_text = "å·²å¯ç”¨" if memory_enabled else "å·²å…³é—­"
        if memory_enabled and metadata.get("conversation_length", 0) > 0:
            memory_status_text += f" - {metadata['conversation_length']}è½®å¯¹è¯"

        return (
            response,  # output
            total_conv,  # total_conversations
            current_sess,  # current_session
            avg_length,  # avg_response_length
            quality,  # quality_score
            time_taken,  # response_time
            docs_num,  # docs_count
            topics_text,  # topics_display
            memory_status_text  # memory_status
        )

    def clear_conversation_handler():
        message = app.clear_memory()
        stats = app.get_conversation_stats()
        return (
            "âœ… å¯¹è¯å†å²å·²æ¸…ç©ºï¼Œå¼€å§‹æ–°çš„å¯¹è¯å§ï¼",
            stats["total_conversations"],
            stats["current_session_length"],
            stats["avg_response_length"],
            0, 0, 0,
            "æš‚æ— è¯é¢˜è®°å½•",
            "å·²å¯ç”¨ - æ–°ä¼šè¯"
        )

    def export_conversation_handler(export_fmt):
        try:
            content = app.export_conversation(export_fmt)
            return content
        except Exception as e:
            return f"å¯¼å‡ºå¤±è´¥: {str(e)}"

    def update_memory_checkbox(memory_enabled):
        status = "å·²å¯ç”¨ - å¯è®°ä½å¯¹è¯å†å²" if memory_enabled else "å·²å…³é—­"
        return status

    # ç»‘å®šäº‹ä»¶
    submit.click(
        fn=process_query,
        inputs=[query, use_memory],
        outputs=[
            output, total_conversations, current_session, avg_response_length,
            quality_score, response_time, docs_count, topics_display, memory_status
        ]
    )

    clear_conversation.click(
        fn=clear_conversation_handler,
        outputs=[
            output, total_conversations, current_session, avg_response_length,
            quality_score, response_time, docs_count, topics_display, memory_status
        ]
    )

    export_btn.click(
        fn=export_conversation_handler,
        inputs=[export_format],
        outputs=[export_output]
    )

    use_memory.change(
        fn=update_memory_checkbox,
        inputs=[use_memory],
        outputs=[memory_status]
    )

    # ç¤ºä¾‹é—®é¢˜
    gr.Examples(
        examples=[
            ["ç°åœ¨å®Œæˆæ—¶çš„ç”¨æ³•æ˜¯ä»€ä¹ˆï¼Ÿ"],
            ["èƒ½ç»™ä¸€äº›å…·ä½“ä¾‹å­å—ï¼Ÿ"],
            ["é‚£å®ƒå’Œè¿‡å»æ—¶æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"],
            ["åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä½¿ç”¨ç°åœ¨å®Œæˆè¿›è¡Œæ—¶ï¼Ÿ"],
            ["å¸®æˆ‘æ€»ç»“ä¸€ä¸‹ç°åœ¨å®Œæˆæ—¶çš„è¦ç‚¹"]
        ],
        inputs=[query]
    )

    # ä½¿ç”¨è¯´æ˜
    with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
        gr.Markdown(
            """
            ### ğŸ§  è®°å¿†åŠŸèƒ½è¯´æ˜

            **å¯ç”¨è®°å¿†**ï¼š
            - åŠ©æ‰‹ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹
            - å¯ä»¥è¿›è¡Œè¿è´¯çš„å¤šè½®å¯¹è¯
            - èƒ½å¤Ÿç†è§£è¿½é—®å’Œä¸Šä¸‹æ–‡å…³è”
            - è‡ªåŠ¨è¿½è¸ªè®¨è®ºçš„è¯é¢˜

            **å…³é—­è®°å¿†**ï¼š
            - æ¯æ¬¡å¯¹è¯éƒ½æ˜¯ç‹¬ç«‹çš„
            - ä¸ä¼šè®°ä½ä¹‹å‰çš„è®¨è®ºå†…å®¹
            - é€‚åˆè¯¢é—®ä¸ç›¸å…³çš„é—®é¢˜

            **å¯¹è¯ç®¡ç†**ï¼š
            - ä½¿ç”¨"æ¸…ç©ºè®°å¿†"æŒ‰é’®é‡æ–°å¼€å§‹
            - å¯ä»¥å¯¼å‡ºå¯¹è¯å†å²è¿›è¡Œå¤ä¹ 
            - ç»Ÿè®¡ä¿¡æ¯å¸®åŠ©äº†è§£å­¦ä¹ è¿›åº¦

            ğŸ’¡ **å»ºè®®**ï¼šå­¦ä¹ ç›¸å…³è¯­æ³•æ—¶ä¿æŒè®°å¿†å¼€å¯ï¼Œè®¨è®ºä¸åŒè¯é¢˜æ—¶å¯ä»¥å…³é—­è®°å¿†ã€‚
            """
        )


# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    print("ğŸ§  æ™ºèƒ½è®°å¿†RAGè‹±è¯­åŠ©æ‰‹å¯åŠ¨ä¸­...")
    demo.launch(share=False, server_name="0.0.0.0", server_port=7865)
    print("âœ… åº”ç”¨å·²å¯åŠ¨: http://localhost:7865")