# app_enhanced.py - å¢å¼ºç‰ˆRAGåº”ç”¨

from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama.llms import OllamaLLM
from retriever_enhanced import EnhancedRetriever
import gradio as gr
from config import LLM_MODEL, QA_PROMPT
import time
from typing import Tuple, Dict, Any


class EnhancedRAGApp:
    """å¢å¼ºç‰ˆRAGåº”ç”¨ï¼Œæ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥å’Œè´¨é‡ç›‘æ§"""

    def __init__(self):
        # 1ï¸âƒ£ å¢å¼ºæ£€ç´¢å™¨
        self.retriever = EnhancedRetriever()

        # 2ï¸âƒ£ LLM
        self.llm = OllamaLLM(model=LLM_MODEL)

        # 3ï¸âƒ£ å¢å¼ºçš„Promptæ¨¡æ¿
        self.prompt = PromptTemplate(
            template=QA_PROMPT,
            input_variables=["context", "query"]
        )

        # 4ï¸âƒ£ åˆ›å»ºå¤šä¸ªå¤„ç†é“¾
        self._setup_chains()

    def _setup_chains(self):
        """è®¾ç½®ä¸åŒçš„å¤„ç†é“¾"""
        # åŸºç¡€QAé“¾
        self.qa_chain = (
            {"context": lambda q: self._get_formatted_docs(q), "query": RunnablePassthrough()}
            | self.prompt
            | self.llm
        )

        # å¢å¼ºQAé“¾ï¼ˆå¸¦æ£€ç´¢è´¨é‡åˆ†æï¼‰
        self.enhanced_qa_chain = self._create_enhanced_chain()

    def _get_formatted_docs(self, query: str, method: str = "enhanced") -> str:
        """è·å–å¹¶æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
        docs = self.retriever.get_relevant_documents(query, method=method)
        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"

        formatted_docs = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get('source', f'æ–‡æ¡£ç‰‡æ®µ{i}')
            content = doc.page_content.strip()
            formatted_docs.append(f"[æ–‡æ¡£{i} - {source}]:\n{content}")

        return "\n\n".join(formatted_docs)

    def _create_enhanced_chain(self):
        """åˆ›å»ºå¢å¼ºçš„å¤„ç†é“¾"""
        def enhanced_process(query: str) -> Tuple[str, Dict[str, Any]]:
            start_time = time.time()

            # æ£€ç´¢æ–‡æ¡£
            docs = self.retriever.get_relevant_documents(query, method="enhanced")
            retrieval_quality = self.retriever.analyze_retrieval_quality(query, docs)

            # å¦‚æœæ£€ç´¢è´¨é‡å¤ªä½ï¼Œç»™å‡ºæç¤º
            if retrieval_quality["quality_score"] < 30:
                return (
                    "âŒ æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å­¦ä¹ èµ„æ–™ã€‚è¯·å°è¯•ï¼š\n"
                    "1. ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\n"
                    "2. æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨è‹±è¯­å­¦ä¹ èŒƒå›´å†…\n"
                    "3. æŸ¥çœ‹çŸ¥è¯†åº“ä¸­æ˜¯å¦åŒ…å«ç›¸å…³å†…å®¹",
                    {"retrieval_quality": retrieval_quality, "response_time": time.time() - start_time}
                )

            # ç”Ÿæˆå›ç­”
            formatted_docs = self._get_formatted_docs(query)
            response = self.qa_chain.invoke(query)

            # æ·»åŠ æ£€ç´¢è´¨é‡ä¿¡æ¯
            quality_note = f"\n\n---\nğŸ“Š æ£€ç´¢è´¨é‡: {retrieval_quality['quality_score']:.1f}/100"
            if retrieval_quality['recommendations']:
                quality_note += f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®: {', '.join(retrieval_quality['recommendations'])}"

            return response + quality_note, {
                "retrieval_quality": retrieval_quality,
                "response_time": time.time() - start_time,
                "num_retrieved_docs": len(docs)
            }

        return enhanced_process

    def chat_with_agent(self, query: str, method: str = "enhanced") -> Tuple[str, Dict[str, Any]]:
        """ä¸æ™ºèƒ½ä½“å¯¹è¯"""
        if not query.strip():
            return "âŒ è¯·è¾“å…¥é—®é¢˜", {}

        try:
            if method == "enhanced":
                # ä½¿ç”¨å¢å¼ºé“¾
                response, metadata = self.enhanced_qa_chain(query)
                return response, metadata
            else:
                # ä½¿ç”¨æŒ‡å®šæ–¹æ³•
                start_time = time.time()
                docs = self.retriever.get_relevant_documents(query, method=method)
                retrieval_quality = self.retriever.analyze_retrieval_quality(query, docs)

                if not docs:
                    return "âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£", {"retrieval_quality": retrieval_quality}

                formatted_docs = self._get_formatted_docs(query, method)
                response = self.qa_chain.invoke(query)

                return response, {
                    "retrieval_quality": retrieval_quality,
                    "response_time": time.time() - start_time,
                    "num_retrieved_docs": len(docs)
                }

        except Exception as e:
            return f"âŒ é”™è¯¯: {str(e)}", {}

    def get_retrieval_debug_info(self, query: str, method: str = "enhanced") -> Dict[str, Any]:
        """è·å–æ£€ç´¢è°ƒè¯•ä¿¡æ¯"""
        docs = self.retriever.get_relevant_documents(query, method=method)
        return self.retriever.analyze_retrieval_quality(query, docs)


# åˆ›å»ºåº”ç”¨å®ä¾‹
app = EnhancedRAGApp()


# Gradioç•Œé¢
with gr.Blocks(title="ğŸ“ è‹±è¯­å­¦ä¹ æ™ºèƒ½ä½“ (å¢å¼ºç‰ˆ)", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ“ è‹±è¯­å­¦ä¹ æ™ºèƒ½ä½“ (å¢å¼ºç‰ˆ)

        > ğŸ’¡ **RAG 2.0** - åŸºäºå¢å¼ºæ£€ç´¢çš„æ™ºèƒ½è‹±è¯­å­¦ä¹ åŠ©æ‰‹

        ğŸš€ **æ–°åŠŸèƒ½**:
        - å¤šç§æ£€ç´¢ç­–ç•¥ (å‘é‡ã€MMRã€æ··åˆ)
        - æ£€ç´¢è´¨é‡ç›‘æ§å’Œåˆ†æ
        - æ™ºèƒ½æ–‡æ¡£é‡æ’åº
        - å®æ—¶æ€§èƒ½æŒ‡æ ‡
        """
    )

    with gr.Row():
        with gr.Column(scale=3):
            query = gr.Textbox(
                label="ä½ çš„é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šç°åœ¨å®Œæˆæ—¶çš„è€ƒç‚¹å’Œç”¨æ³•",
                lines=3
            )
            method = gr.Radio(
                choices=[
                    ("ğŸ§  æ™ºèƒ½å¢å¼º", "enhanced"),
                    ("ğŸ“Š å‘é‡ç›¸ä¼¼æ€§", "vector"),
                    ("ğŸ”„ å¤šæ ·æ€§æ£€ç´¢ (MMR)", "mmr"),
                    ("ğŸ” æ··åˆæ£€ç´¢", "ensemble")
                ],
                value="enhanced",
                label="æ£€ç´¢æ–¹æ³•",
                info="é€‰æ‹©ä¸åŒçš„æ£€ç´¢ç­–ç•¥"
            )
            with gr.Row():
                submit = gr.Button("ğŸš€ æé—®", variant="primary")
                debug_btn = gr.Button("ğŸ” æ£€ç´¢è°ƒè¯•", variant="secondary")

        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“ˆ æ£€ç´¢ç»Ÿè®¡")
            quality_score = gr.Number(label="è´¨é‡è¯„åˆ†", precision=1, interactive=False)
            response_time = gr.Number(label="å“åº”æ—¶é—´(ç§’)", precision=2, interactive=False)
            doc_count = gr.Number(label="æ£€ç´¢æ–‡æ¡£æ•°", precision=0, interactive=False)

    # ä¸»å›ç­”åŒºåŸŸ
    output = gr.Markdown(label="æ™ºèƒ½å›ç­”", height=300)

    # è°ƒè¯•ä¿¡æ¯åŒºåŸŸ
    with gr.Accordion("ğŸ” æ£€ç´¢è°ƒè¯•ä¿¡æ¯", open=False):
        debug_info = gr.JSON(label="è¯¦ç»†æ£€ç´¢åˆ†æ")

    # ç»‘å®šäº‹ä»¶
    def process_query(query_text, method_choice):
        response, metadata = app.chat_with_agent(query_text, method_choice)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        quality = metadata.get("retrieval_quality", {}).get("quality_score", 0)
        time_taken = metadata.get("response_time", 0)
        num_docs = metadata.get("num_retrieved_docs", 0)

        return response, quality, time_taken, num_docs

    def get_debug_info(query_text, method_choice):
        debug_data = app.get_retrieval_debug_info(query_text, method_choice)
        return debug_data

    submit.click(
        fn=process_query,
        inputs=[query, method],
        outputs=[output, quality_score, response_time, doc_count]
    )

    debug_btn.click(
        fn=get_debug_info,
        inputs=[query, method],
        outputs=[debug_info]
    )

    # ç¤ºä¾‹é—®é¢˜
    gr.Examples(
        examples=[
            "ç°åœ¨å®Œæˆæ—¶çš„è€ƒç‚¹å’Œç”¨æ³•",
            "å¦‚ä½•æ­£ç¡®ä½¿ç”¨å† è¯ a/an/the",
            "è‹±è¯­é˜…è¯»ç†è§£çš„è§£é¢˜æŠ€å·§",
            "è™šæ‹Ÿè¯­æ°”çš„è¯­æ³•è§„åˆ™",
            "å®šè¯­ä»å¥çš„ä½¿ç”¨æ–¹æ³•"
        ],
        inputs=[query]
    )


# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=7863)
    print("ğŸš€ å¢å¼ºç‰ˆRAGåº”ç”¨å·²å¯åŠ¨: http://localhost:7863")