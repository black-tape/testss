# build_knowledge_base.py

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
import os
from config import DOCS_DIR, DB_DIR, EMBEDDING_MODEL, CHUNK_SIZE, CHUNK_OVERLAP


def load_and_index_documents():
    docs = []

    for filename in os.listdir(DOCS_DIR):
        path = os.path.join(DOCS_DIR, filename)

        if filename.endswith(".pdf"):
            loader = PyPDFLoader(path)
        elif filename.endswith(".docx"):
            loader = Docx2txtLoader(path)
        elif filename.endswith(".txt"):
            loader = TextLoader(path)
        else:
            print(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
            continue

        docs.extend(loader.load())

    if not docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨æ–‡æ¡£ï¼Œè¯·åœ¨ docs/ æ–‡ä»¶å¤¹ä¸­æ·»åŠ æ•™ææˆ–ç¬”è®°ã€‚")
        return

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    split_docs = splitter.split_documents(docs)
    print(f"ğŸ“š å·²åŠ è½½ {len(split_docs)} ä¸ªæ–‡æ¡£å—")

    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
    db = FAISS.from_documents(split_docs, embeddings)
    db.save_local(DB_DIR)

    print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œä¿å­˜äº: {DB_DIR}")


if __name__ == "__main__":
    load_and_index_documents()
